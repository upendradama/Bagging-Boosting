# AdaBoosting and XGBoosting

### Problem Statement :- 

  - Perform AdaBoost and Extreme Gradient Boosting for the WBCD dataset

### Data Understanding

```{r}
library(readr)
wbcd <- read_csv("/Users/thanush/Desktop/Digi 360/Module 20/Datasets-9/wbcd.csv")
head(wbcd)
```

```{r}
wbcd <- wbcd[-c(1)]
head(wbcd)
```

```{r}
#converting features into factor
wbcd$diagnosis <- as.factor(wbcd$diagnosis)
str(wbcd)
```

```{r}
library(caret)
library(tibble)
set.seed(1000)
index <- createDataPartition(y = wbcd$diagnosis, p = 0.7, list = FALSE)
train <- wbcd[index, ]
test <- wbcd[-index, ]
```

```{r}
head(train)
```




### Extreme Gradient Boosting

To use xgboost package, keep these things in mind:
  - Convert the categorical variables into numeric using one hot encoding
  - For classification, if the dependent variable belongs to class factor, convert it to numeric

```{r}
#convert data frame to data table
library(data.table)
library(mlr)
setDT(train) 
setDT(test)
```

```{r}
#using one hot encoding 
train_label <- train$diagnosis 
test_label <- test$diagnosis
```

```{r}
train_label
```


```{r}
new_train <- model.matrix(~.+0,data = train[,-c("diagnosis"),with=F]) 
new_test <- model.matrix(~.+0,data = test[,-c("diagnosis"),with=F])
```

```{r}
#convert factor to numeric 
train_label <- as.numeric(train_label)-1
test_label <- as.numeric(test_label)-1
```

```{r}
train_label
```

For xgboost, we'll use xgb.DMatrix to convert data table into a matrix 

```{r}
library(xgboost)
library(dplyr) 
#preparing matrix 
dtrain <- xgb.DMatrix(data = new_train,label = train_label) 
dtest <- xgb.DMatrix(data = new_test,label=test_label)
```

### Building Model with Default Parameters

```{r}
#default parameters
params <- list(booster = "gbtree", objective = "binary:logistic", eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)
```

Using the inbuilt xgb.cv function, let's calculate the best nround for this model. 

### Cross Validation

```{r}
set.seed(1000)
xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5, showsd = T, stratified = T, print_every_n = 10, early_stopping_rounds = 20, maximize = F)

```

The model returned lowest error at the 71st (nround) iteration. Also, if you noticed the running messages in your console, you would have understood that train and test error are following each other. 

### Training the Model

```{r}
#first default - model training
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 71, watchlist = list(val=dtest,train=dtrain), print_every_n = 10, early_stoppin_rounds = 10, maximize = F , eval_metric = "error")
```


### Model Evolution

```{r}
#model prediction
xgbpred <- predict (xgb1,dtest)
xgbpred <- ifelse (xgbpred > 0.5,1,0)
```

The objective function binary:logistic returns output probabilities rather than labels. To convert it, we need to manually use a cutoff value. As seen above, I've used 0.5 as my cutoff value for predictions. We can calculate our model's accuracy using confusionMatrix() function from caret package.

```{r}
#confusion matrix
library(caret)
confusionMatrix(as.factor(xgbpred), as.factor(test_label))
```

The accuracy of the model is 96%.

### Feature Importance

```{r}
#view variable importance plot
mat <- xgb.importance (feature_names = colnames(new_train),model = xgb1)
xgb.plot.importance (importance_matrix = mat[1:20]) 
```


### Adaboosting

```{r}
library(parallel)
library(parallelMap) 
library(mlr)
library(dplyr) 
library(caret) 
library(data.table)
```


```{r}
# Let's build the model on train data
parallelStartSocket(cpus = detectCores())
```

```{r}
library(adabag)
library(rpart)
set.seed(1000)
model = boosting(diagnosis~., data=train, boos=TRUE, mfinal=10, coeflearn = 'Zhu',control=rpart.control(maxdepth=3))
```


```{r}
print(names(model))
```

```{r}
print(model$trees[1])
```
### Model Evolution

```{r}
pred = predict(model, test)
```

```{r}
print(pred$confusion)
```

```{r}
accuracy = (104 + 61)/(104 + 61 + 3 + 2)
accuracy
```

### Cross Validation Model

```{r}
set.seed(1000)
cvmodel = boosting.cv(diagnosis~., data=train, boos=TRUE, mfinal=10, v=5)
parallelStop()
```

```{r}
cvmodel
```


```{r}
accuracy = (242 + 137)/(242 + 137 + 8 + 12)
accuracy
```


### Conclusion:- 

  - With Adaboosting the accuracy is 97%.
  - With Adaboosting cross validation the accuracy is 95%
  - With XGBoosting the accuracy is 96%.
  - The most important features are `perimeter_worst`, `points_mean`, `area_worst`, `points_worst` and `radius_worst`
